
\section{Derivatives, Part III (Consequences)}

This chapter builds up to two new capabilities. The first is graph
sketching (or put differently, using features of the derivative to
approximate shapes of functions). The second is L'H\^opital's rule,
which allows us to take limits of the form $\frac{0}{0}$. We build up
to these capabilities by studying the consequences of the derivative.
On the way we prove the Mean Value Theorem, one of key calculus
results.

\vs

I reordered content from Spivak's chapter for clarity, but kept the
theorem and corollary numbering scheme. Thus theorem numbers here
aren't in order, but they match the numbers in Spivak.\footnote{This
  was the most meandering of Spivak's chapters; it took \textit{a lot}
  of reordering for the overall structure to make sense. Not sure if
  it's the material, the presentation, or just me losing steam, but
  getting through this chapter took forever. By the end of it I was
  bored to tears, barely able to force myself to finish it.}

\subsection{Maxima and manima}
We start with some definitions. Let $f$ be a function and $A$ a set of
numbers contained in $f$'s domain.\footnote{$A$ need not have any
  additional properties. E.g. it may have holes, etc.} Then:

\vs

\textbf{Definition.} A point $x$ in $A$ is a \textbf{maximum
  point} for $f$ on $A$ if
\[f(x)\geq f(y)\qquad\text{for every $y$ in $A$}\]

\textbf{Definition.} A \textbf{critical point} of $f$ is a number $x$
such that $f'(x)=0$.\footnote{If $x$ is a maximum and/or critical
  point, then $f(x)$ is called a maximum and/or critical value of
  $f$.}

\vs---\vs

\textbf{Theorem 1a.} Let $f$ be any function defined on $(a,b)$. If $x$
is a maximum point for $f$ on $(a,b)$, and $f$ is differentiable at
$x$, then $f'(x)=0$.

\vs

\textit{Intuitively,} maximum and minimum points are also critical
points (but \textbf{not} the other way around-- $f(x)=x^3$ has
$f'(0)=0$ as an obvious counterexample\footnote{In this case this
  critical point is called the \textbf{saddle point}.}).

\vs

\textbf{Proof.} \textit{Informally}, suppose $a$ is a maximum point.
Draw a secant line between $a$ and $a_l$ (to its left), and another
line between $a$ and $a_r$ (to its right). The $a-a_l$ line will slope
up, the $a-a_r$ line will slope down. Thus at $a$ the slope crosses
from positive to negative, and is $0$.

\vs

\textit{Formally}, let $h\in\mathcal{R}$ such that $x+h\in(a,b)$. If $h<0$ it follows that:
\begin{align*}
  &f(x+h)\leq f(x)&\text{since $f(x)$ is a maximum value}\\
  &\implies f(x+h)-f(x)\leq 0\\
  &\implies\frac{f(x+h)-f(x)}{h}\geq0&\text{dividing by negative $h$}\\
  &\implies \lim_{h\to0^-}\frac{f(x+h)-f(x)}{h}\geq0&\text{see
                                                 \ref{subsec:onesided-limits}
                                                 and
                                                 \ref{subsubsec:nonzero-lemma}}
\end{align*}

Conversely, if $h>0$ it follows that:
\begin{align*}
  &\implies \frac{f(x+h)-f(x)}{h}\leq0&\text{dividing by positive $h$}\\
  &\implies \lim_{h\to0^+}\frac{f(x+h)-f(x)}{h}\leq0
\end{align*}

By hypothesis, $f$ is differentiable at $x$. Thus the two limits must
be equal to each other, and to $f'(x)$. Therefore $f'(x)\geq0$ and
$f'(x)\leq0$. Thus $f'(x)=0$ as desired.

\vs

\textbf{Theorem 1b.} Let $f$ be any function defined on $(a,b)$. If
$x$ is a \textit{minimum} point for $f$ on $(a,b)$, and $f$ is
differentiable at $x$, then $f'(x)=0$.

\vs

\textbf{Proof.} Let $g=-f$. Then $x$ is a maximum point of $g$. By 1a,
$g'(x)=0$, thus $(-f)'(x)=-1\cdot f'(x)=0$, and thus $f'(x)=0$ as desired.

\vs---\vs

The obvious (extremely valuable) consequences of these theorems is
that we can find minimum and maximum values of $f$ by solving for
$f'(x)=0$.

\subsubsection*{On closed intervals}
If $x$ is a maximum or minimum point for $f$ on $[a,b]$, then $x$ must
be in one of three classes:
\begin{enumerate}
\item The critical points of $f$ in $[a,b]$.
\item The end points $a$ and $b$.
\item Points $x\in[a,b]$ such that $f$ is not differentiable at $x$.
\end{enumerate}

(If $x$ is not in second or third group, then $x\in(a,b)$ and $f'(x)$
exists; thus $f'(x)=0$ by Theorem 1, and thus $x$ is in the first
group.)

\subsubsection*{On open intervals}

On open intervals we don't necessarily know minimum or maximum values
exist, so these problems require creative work. For example, recall we
showed even degree polynomials have a minimum value (see
\ref{even-poly-root}). Now that we've done that work for even $n$ and
$f(x)=x^{n}+a_{n-1}x^{n-1}+\ldots+a_{0}$, we can solve for $x$ in
$f'(x)=0$ to find minimal values. But because the interval is open, we
had to do the work in \ref{even-poly-root} to show minimal value
exists first.

\subsection{Mean Value Theorem}

\textbf{Theorem 3 (Rolle's theorem).} Let $f$ be continuous on $[a,b]$
differentiable on $(a,b)$, and let $f(a)=f(b)$. Then there exists
$x\in(a,b)$ such that $f'(x)=0$.

\vs

\textbf{Proof.} There are two cases:
\begin{itemize}
\item \textit{Case 1.} Suppose the maximum or the minimum occurs at a
  point $x\in(a,b)$. Then $f'(x)=0$ by theorem 1, and we are done.
\item \textit{Case 2.} Suppose the maximum and the minimum both occur
  at endpoints. Since $f(a)=f(b)$, the maximum and the minimum values
  are equal and $f$ is constant. Then for any $x\in(a,b), f'(x)=0$ and
  we are done.
\end{itemize}

\vs---\vs

\textbf{Claim:} Let $f(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0$. Then
$f$ has at most $n$ roots.

\vs

\textbf{Proof.} TODO


\vs---\vs

\textbf{Claim:} Let $f(x)=a_nx^n+a_{n-1}x^{n-1}+\ldots+a_0$. Then
$f$ has at most $n-1$ critical points.

\vs

\textbf{Proof.} This is easy. Observe that
\[f'(x)=a_n n x^{n-1} + a_{n-1} (n-1) x^{n-2} + \dots + a_1\] Thus by
previous claim, $f'$ has at most $n-1$ roots, and thus at most $n-1$
critical points.

\vs---\vs

\textbf{Theorem 4 (Mean value theorem).}

Let $f$ be continuous on $[a,b]$ and differentiable on $(a,b)$. Then
there exists $x\in(a,b)$ such that:
\[f'(x)=\frac{f(b)-f(a)}{b-a}\]

Here are three intuitions:
\begin{enumerate}
\item \textit{Geometric intuition.} There exists a line tangent to $f$
  parallel to the line between the endpoints (i.e. line between
  $(a, f(a))$ and $(b, f(b))$).
\item \textit{Algebraic intuition.} There exists a point $x$ at which
  instantaneous rate of change of $f$ is equal to the average change
  of $f$ on $[a,b]$.
\item \textit{Physical example.} If you travel 60 miles in one hour,
  at some point you must have been travelling exactly 60 miles per
  hour.
\end{enumerate}

\textbf{Proof.}

Here's an informal proof outline:
\begin{itemize}
\item Take the line segment formed by endpoints $(a,f(a))$ and
  $(b,f(b))$.
\item Construct a function $g$ that for $x\in(a,b)$ returns the vertical
  distance between $f(x)$ and the line segment. (We'll show it's
  continuous and differentiable.)\footnote{It turns out not to matter
    whether $g$ computes the distance between $f$ and the line
    segment, or $f$ and the line segment shifted down by $f(a)$ (i.e.
    down to $x$-axis). So in practice we use the lattern form to avoid
    dealing with the $f(a)$ term in the linear equation.}
\item By Rolle's theorem, it has a flat tangent. It's easy to show
  algebraically (and visualize geometrically) this proves the MVT.
\end{itemize}

Formally, let\footnote{See \ref{subsubsec-point-slope-form} for how
  the point-slope form is used to construct the second term.}
\[h(x)=f(x)-\left[\frac{f(b)-f(a)}{b-a}(x-a)\right]\]

Observe $h$ is continuous on $[a,b]$ and differentiable on $(a,b)$.
Further:
\begin{align*}
  h(a)&=f(a)-\left[\frac{f(b)-f(a)}{b-a}\cdot0\right]=f(a)\\
  h(b)&=f(b)-\left[\frac{f(b)-f(a)}{b-a}(b-a)\right]\\
      &=f(b)-[f(b)-f(a)]\\
      &=f(a)
\end{align*}

Thus we can apply Rolle's Theorem $h$ to conclude there is $x\in(a,b)$
such that:\footnote{Note the derivative of a line is its slope, thus
  $\frac{d}{dx}\left[\frac{f(b)-f(a)}{b-a}(x-a)\right]=\frac{f(b)-f(a)}{b-a}$.}

\begin{align*}
  &0=h'(x)=f'(x)-\frac{f(b)-f(a)}{b-a}\\
  &\implies f'(x)=\frac{f(b)-f(a)}{b-a}
\end{align*}

QED.

\vs---\vs

\textbf{MVT Corollary 1.} If $f$ is defined on an interval and $f'(x)=0$
for all $x$ in the interval, then $f$ is constant on the interval.

\vs

\textit{Intuitively,} if the velocity of a particle is always zero,
the particle must be standing still.

\vs

\textbf{Proof.} Let $a\neq b$ be any two points on the interval. Then
there is $x\in(a,b)$ such that $f'(x)=\frac{f(b)-f(a)}{b-a}$. But
$f'(x)=0$ for all $x$ on the interval, thus $0=\frac{f(b)-f(a)}{b-a}$.
Thuf $f(a)=f(b)$ for any $a,b$ (i.e. $f$ is constant on the interval
as desired).

\vs---\vs

\textbf{MVT Corollary 2.} If $f,g$ are defined on the same interval, and
$f'(x)=g'(x)$ for all $x$ in the interval, then there is
$c\in\mathcal{R}$ such that $f=g+c$.

\vs

\textbf{Proof.} Observe that
\begin{align*}
  &f'(x)=g'(x)\\
  &\implies f'(x)-g'(x)=0\\
  &(f-g)'(x)=0
\end{align*}

By corollary 1, $(f-g)$ is constant, i.e. $f=g+c$ as desired.

\subsection{Increasing and decreasing functions}

\textbf{Definition.} A function is \textbf{increasing} on an interval
if $f(a)<f(b)$ whenever $a,b$ are two numbers in the interval with
$a<b$.\footnote{The decreasing function definition is obvious.}

\vs

\textbf{MVT Corollary 3a.} If $f'(x)>0$ for all $x$ on an interval, then
$f$ is increasing on the interval.

\vs

\textbf{Proof.} Let $a<b$ be two points on an interval. Then there
exists $x\in(a,b)$ such that
\[f'(x)=\frac{f(b)-f(a)}{b-a}\]

But $f'(x)>0$ for all $x\in(a,b)$, thus
\[\frac{f(b)-f(a)}{b-a}>0\]

We know $b-a>0$, thus $f(b)>f(a)$ as desired.

\vs---\vs

\textbf{MVT Corollary 3b.} If $f'(x)<0$ for all $x$ on an interval, then
$f$ is decreasing on the interval.

\vs

\textbf{Proof.} The proof is an obvious modification of 3a.

\subsection{Local maxima and manima}
\textbf{Definition.} A point $x$ in $A$ is a \textbf{local maximum
  point} for $f$ on $A$ if there is some $\delta>0$ such that $x$ is a
maximum point for $f$ on $A\cap(x-\delta, x+\delta)$.

\vs

\textbf{Theorem 2.} If $f$ is defined on $(a,b)$ and has a local
maximum (or minimum) at $x$, and $f$ is differentiable at $x$, then
$f'(x)=0$.

\vs

\textbf{Proof.} The proof is a trivial application of theorem 1 to $f$
on $(x-\delta, x+\delta)$.

\vs---\vs

The opposite of theorem 2 isn't true-- if $f'(x)=0$ it does not
necessarily imply there is a local minimum or maximum at $x$ (consider
$f(x)=x^3$ at $x=0$ for example). Thus if we want to determine if a
point is a local maximum, we cannot use theorem 2 to make that
determination. We need some other way. The derivative offers us two
such tests to choose between.

\subsubsection*{First derivative test}

Suppose $f'(x)=0$. Check the sign of $f'$ on some interval to the
left and right of $x$. Then:
\begin{itemize}
\item If $f'>0$ to the left of $x$ and $f'<0$ to the right, then $x$
  is a local maximum (since $f$ is increasing to the left and
  decreasing to the right of $x$).
\item If $f'<0$ to the left of $x$ and $f'>0$ to the right, then $x$
  is a local minimum (since $f$ is decreasing to the left and
  increasing to the right of $x$).
\item Otherwise $x$ is a saddle point.
\end{itemize}

\subsubsection*{Second derivative test}

\textbf{Theorem 5.} Suppose $f'(a)=0$. If $f''(a)>0$, then $f$ has a
local minimum at $a$; if $f''(a)<0$, then $f$ has a local maximum at
$a$.

\vs

\textit{Intuitively,} $f''(a)>0$ means the slope of $f$ (represented
by $f'$) is increasing around $a$. Thus the curve must be sloping
upward.

\vs

\textbf{Proof.}
\begin{align*}
  f''(a)&=\lim_{h\to0}\frac{f'(a+h)-f'(a)}{h}&&\text{by derivative definition}\\
        &=\lim_{h\to0}\frac{f'(a+h)}{h}&&\text{$f'(a)=0$ by hypothesis}\\
\end{align*}

Suppose $f''(a)>0$. By nonzero neighborhood lemma (see
\ref{subsubsec:nonzero-lemma}), $f'(a+h)/h>0$ for sufficiently small
$h$. Therefore, for sufficiently small $h$:
\begin{itemize}
\item $f'(a+h)>0$ when $h>0$
\item $f'(a+h)<0$ when $h<0$
\end{itemize}

Thus by Corollary 3 of MVT:
\begin{itemize}
\item $f$ is increasing in some interval to the right of $a$
\item $f$ is decreasing in some interval to the left of $a$
\end{itemize}

Thus $f$ has a local minimum at $a$, as desired\footnote{The proof for
  $f''(a)<0$ is similar.}

\vs

(Note that theorem 5 gives no information if $f''(a)=0$.)

\vs---\vs

\textbf{Theorem 6.} Suppose $f''(a)$ exists. If $f$ has a local
minimum at $a$, then $f''(a)\geq0$; if $f$ has a local maximum at $a$,
then $f''(a)\leq 0$.

\vs

\textit{Intuitively,} this is a partial converse of thereom 5.

\vs

\textbf{Proof.} Suppose $f$ has a local minimum at $a$. Suppose for
contradiction $f''(a)<0$. Then by theorem 5 $f$ also has a local
maximum at $a$. Thefore $f$ is constant in some interval around $a$,
and so $f''(a)=0$. We have a contradiction, thus $f(a)\geq 0$, as
desired.\footnote{The proof for $f''(a)>0$ is similar.}


\subsection{Graph sketching}
We've now developed all the tools to get our first capability-- using
derivatives to approximate the shapes of graphs. Putting everything
together, to sketch a graph of a ``reasonable'' function $f$, follow
the following steps.

\vs

First of all, check if the function is even, or is odd.\footnote{Even
  functions are of the form $f(-x)=f(x)$ (i.e. symmetric about the
  $y$-axis). Odd functions are of the form $f(-x)=-f(x)$ (i.e.
  symmetric about the origin).} Symmetries can save a lot of time!
Then:

\begin{enumerate}
\item Express the domain of $f$ as a union of intervals.
\item Find limits as $f$ approaches \textit{open}
  endpoints.\footnote{Usually via one-sided and infinite limits, and
    limits at infinity.}
\item Find $f'$. Identify critical points (where $f'(x)=0$),
intervals where $f'>0$ (so $f$ is increasing), and intervals where
$f'<0$ (so $f$ is decreasing).
\item Use the first derivative test to identify local minima and
  maxima.
\item Plot obvious points (intercepts of axes, local minima and
  maxima, and points where the derivative doesn't exist). Interpolate
  the graph between them!
\end{enumerate}

\subsubsection*{Example 1}
First example is $f(x)=x^3+3x^2-9x+12$.

\begin{enumerate}
\item
\item 
\item 
\item 
\item 
\end{enumerate}

\subsubsection*{Example 2}
Second example is $f(x)=\frac{x^2}{1-x^2}$.

\begin{enumerate}
\item
\item 
\item 
\item 
\item 
\end{enumerate}

\subsection{L'H\^opital's rule}
We will build up to L'H\^opital's rule in the following way:

\begin{enumerate}
\item First, we'll cover a special case of L'H\^opital's rule (theorem
  7, no holes in derivatives).
\item To prove the L'H\^opital's rule we'll need Cauchy's Mean Value
  Theorem (a generalization of the Mean Value Theorem). When stated
  outright it can be hard to parse, so we'll build up to it informally
  next.
\item Formally prove Cauchy's Mean Value Theorem.
\item Prove L'H\^opital's rule.
\item We then reprove theorem 7 in a simpler way using L'H\^opital's
  rule.
\end{enumerate}

---\vs

\textbf{Theorem 7.} Suppose (1) $f$ is continuous at $a$, (2) $f'(x)$
exists for all $x$ in $0<|x-a|<\delta$, and (3) $\lim_{x\to a}f'(x)$ exists.
Then $f'(a)$ exists and
\[f'(a)=\lim_{x\to a}f'(x)\]

\textit{Intuitively,} derivatives cannot have holes. Put differently,
$f'$ \textit{can} be discontinuous at $a$ by fluctuating wildly near
$a$\footnote{In \ref{subsec-sine-poly} we've already seen an example
  of this:
  \[f(x)=\begin{cases}
    x^2\sin \frac{1}{x},&x\neq0\\
    0,&x=0.
  \end{cases}\]}, but
not by being undefined at $a$, or by being defined at $a$ to be far from its limit
near $a$.

\vs

\textbf{Proof 1.}\footnote{We will give a second proof in terms of
  L'H\^opital's rule at the end of this chapter.} Informally, for
``nice'' functions like $f$, the mean value theorem applies on tiny
scales of $\delta-\epsilon$ limits.

\vs

By derivative definition
\[f'(a)=\lim_{h\to 0}\frac{f(a+h)-f(a)}{h}\]

For sufficiently small $h$, both positive and negative, by supposition:
\begin{itemize}
\item $f$ will be continuous on $[a,a+h]$
\item $f$ will be differentiable on $(a,a+h)$
\end{itemize}

These conditions are sufficient for the mean value theorem, and thus
there exists $\alpha_h\in(a, a+h)$ such that:
\[\frac{f(a+h)-f(a)}{h}=f'(\alpha_h)\]

Putting this together:\footnote{Spivak observes the last equation in
  the proof is handwavy and needs a proper $\delta-\epsilon$ proof. I need to move
  on, so leaving this as a TODO.}
\begin{align*}
  f'(a)&=\lim_{h\to0}\frac{f(a+h)-f(a)}{h}&\text{by derivative definition}\\
       &=\lim_{h\to0}f'(\alpha_h)&\text{by mean value theorem}\\
       &=\lim_{x\to a}f'(x)&\text{$\alpha_h\in(a,a+h)$, thus as $h\to0$, $\alpha_h\to a$}
\end{align*}

\vs---\vs

\textbf{Theorem 8 (Cauchy's MVT), handwavy version.}

\vs

Let $f,g$ be continuous on $[a,b]$ and differentiable on $(a,b)$.
\textit{Intuitively,} theorem 8 states that there exists a point
$x\in(a,b)$ where $\frac{f'(x)}{g'(x)}$ (i.e. the ratio of
\textit{instantaneous} changes of $f$ and $g$) is the same as the
ratio of \textit{average} changes of $f$ and $g$ on $[a,b]$.

\vs

A more formal version of this is:
\begin{align*}
  \frac{f'(x)}{g'(x)}&=\frac{f(b)-f(a)}{b-a}\div \frac{g(b)-g(a)}{b-a}\\
                     &=\frac{f(b)-f(a)}{b-a}\cdot \frac{b-a}{g(b)-g(a)}\\
                     &=\frac{f(b)-f(a)}{g(b)-g(a)}
\end{align*}

when $g'(x)\neq 0$ and $g(b)-g(a)\neq0$.

\vs

There are two additional considerations. First, if $g(x)=x$ then
$g'(x)=1$, and the theorem simplifies to $f'(x)=\frac{f(b)-f(a)}{b-a}$
(i.e. we obtain the mean value theorem).

\vs

Second, to avoid division by zero constraints, formally the Cauchy
theorem is expressed as a multiplication rather than division of
terms:

\begin{align*}
  &\frac{f'(x)}{g'(x)}=\frac{f(b)-f(a)}{g(b)-g(a)}\\
  &\implies [f(b)-f(a)]g'(x)=[g(b)-g(a)]f'(x)
\end{align*}

With this buildup, we're ready to prove Cauchy's MVT.

\vs

\textbf{Theorem 8 (Cauchy's MVT).} Let $f,g$ be continuous on $[a,b]$
and differentiable on $(a,b)$. Then there exists $x\in(a,b)$ such that
\[[f(b)-f(a)]g'(x)=[g(b)-g(a)]f'(x)\]

\vs

\textbf{Proof.} Let
\[h(x)=f(x)[g(b)-g(a)]-g(x)[f(b)-f(a)]\]

Then $h$ is continuous on $[a,b]$ and differentiable on $(a,b)$.
Further observe that\footnote{If you plug $a$ and $b$ into $h(x)$, I
  promise this works (I checked).}
\[h(a)=f(a)g(b)-f(b)g(a)=h(b)\]

Thus $h(a)=h(b)$, Rolle's theorem applies, and there exists
$x\in(a,b)$ such that $h'(x)=0$. Taking the derivative of
$h$\footnote{Note to derive $h'$ we treat $g(b)-g(a)$ and $f(b)-f(a)$
  as constants.}:
\[0=h'(x)=f(x)'[g(b)-g(a)]-g(x)'[f(b)-f(a)]\]

---\vs

\textbf{Theorem 9 (L'H\^opital's rule.)} Suppose that
\[\lim_{x\to a}f(x)=0\qquad\text{and}\qquad\lim_{x\to a}g(x)=0\]

Then\footnote{Of course assuming $\lim_{x\to a}\frac{f'(x)}{g'(x)}$ exists.}
\[\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{x\to a}\frac{f'(x)}{g'(x)}\]

\textbf{Proof.} Recall Cauchy's MVT states that assuming $f,g$ are
continuous on $[a,b]$ and differentiable on $(a,b)$, there exists
$x\in(a,b)$ such that
\[\frac{f'(x)}{g'(x)}=\frac{f(b)-f(a)}{g(b)-g(a)}\]

\textit{Informally}, if we can make $f(a)=g(a)=0$, this equation will
start to look vaguely close to what we need. Assuming we can make it
work, we can then take limits of both sides to prove the L'H\^opital's
rule. This gives us a very rough outline of the proof.
\begin{enumerate}
\item For Cauchy's MVT to work we need to show continuity of $f$ and
  $g$.
\item ``Vaguely'' looking like what we need is not enough. We need to
  figure out how to map L'H\^opital's to Cauchy's MVT exactly.
\item To divide we need to show $g'(x)\neq0$ and $g(b)\neq0$.
\item Finally, we must take limits of both sides.
\end{enumerate}

We now proceed with formalizing each step.

\vs

\textit{Step 1:} Show $f,g$ are continuous.

\vs

By hypothesis $\lim_{x\to a}f(x)=0$ and $\lim_{x\to a}g(x)=0$. We don't
know the definitions of $f(a)$ and $g(a)$. But since in L'H\^opital's
rule we only care about limits, redefining $f(a)$ or $g(a)$ will not
affect the hypothesis. Thus, redefine $f(a)=g(a)=0$. Since now
$\lim_{x\to a}f(x)=f(a)$ and $\lim_{x\to a}g(x)=g(a)$, it follows
$f,g$ are now continuous.

\vs

\textit{Step 2:} Map L'H\^opital's rule to Cauchy's MVT.

\vs

Further by hypothesis, $\lim_{x\to a}f'(x)/g'(x)$ exists. By limit
definition $f'(x)$ and $g'(x)$ exist in the neighborhood of
$x\in(a-\delta, a+\delta)$ with the possible exception of $x=a$. Let's consider
all $x\in(a-\delta, a)$ and $x\in(a, a+\delta)$ separately.

\vs

For all $x\in(a-\delta, a)$ consider $f$ and $g$ on interval $[x,a]$. Both
are continuous on $[x,a]$ and differentiable on $(x,a)$, thus Cauchy's
MVT applies, i.e. there exists $\alpha\in(x,a)$ such that:
\begin{align*}
  &[f(a)-f(x)]g'(\alpha)=[g(a)-g(x)]f'(\alpha)\\
  &\implies -f(x)g'(\alpha)=-g(x)f'(\alpha)&&\text{since $f(a)=g(a)=0$}\\
  &\implies f(x)g'(\alpha)=g(x)f'(\alpha)&&\text{multiply both sides by $-1$}
\end{align*}

Similarly, for all $x\in(a, a+\delta)$ consider $f$ and $g$ on interval $[a,x]$. Both
are continuous on $[a,x]$ and differentiable on $(a,x)$, thus Cauchy's
MVT applies, i.e. there exists $\alpha\in(a,x)$ such that:
\begin{align*}
  &[f(x)-f(a)]g'(\alpha)=[g(x)-g(a)]f'(\alpha)\\
  &\implies f(x)g'(\alpha)=g(x)f'(\alpha)&&\text{since $f(a)=g(a)=0$}
\end{align*}

\textit{Step 3:} Show $g'(\alpha)\neq0$ and $g(x)\neq0$.

\vs

It is easy to see $g'(\alpha)\neq0$. This is true because by hypothesis
$\lim_{x\to a}f'(x)/g'(x)$ exists for all $x\in(a-\delta, a+\delta)$ except possibly
$x=a$, thus $g'(x)\neq0$.

\vs

We have left to prove $g(x)\neq0$. Suppose for contradiction $g(x)=0$.
Apply mean value theorem to $g$ on $[a,x]$. Recall we defined
$g(a)=0$, thus there exists $x_1\in[a,x]$ such that $g'(x_1)=0$. This
contradicts the paragraphaph above. The same argument applies to
interval $[x,a]$.

\vs

Therefore:
\begin{align*}
  &f(x)g'(\alpha)=g(x)f'(\alpha)\\
  &\implies \frac{f(x)}{g(x)}=\frac{f'(\alpha)}{g'(\alpha)}
\end{align*}


\textit{Step 4:} Take limits of both sides.

\vs

Recall that $\alpha\in(a,x)$ or $\alpha\in(x,a)$. Thus as $x$ approaches $a$, $\alpha$
approaches $a$. Therefore
\[\lim_{x\to a}\frac{f(x)}{g(x)}=\lim_{\alpha\to a}\frac{f'(\alpha)}{g'(\alpha)}\]
as desired.\footnote{This is a complicated proof, and I don't think
  Spivak does a particularly good job of it. I should maybe go back
  and clean this up when I can look at it again with fresh eyes. Lots
  of sloppiness here. But I spent enough time on it so for now ready
  to move on.}

\vs---\vs

\textbf{Theorem 7, proof 2.} We can now offer a second proof for
Theorem 7, as it turns out to be a special case of L'H\^opital's rule.

\vs

Recall, the theorem asserts the following. Suppose (1) $f$ is
continuous at $a$, (2) $f'(x)$ exists for all $x$ in $0<|x-a|<\delta$, and
(3) $\lim_{x\to a}f'(x)$ exists. Then $f'(a)$ exists and
\[f'(a)=\lim_{x\to a}f'(x)\]

\vs

By derivative definition:
\[f'(a)=\lim_{h\to0}\frac{f(a+h)-f(a)}{h}\]

Let $x=a+h$. We can rewrite the equation above as follows:
\[f'(a)=\lim_{x\to a}\frac{f(x)-f(a)}{x-a}\]

Clearly $\lim_{x\to a}[f(x)-f(a)]=0$ and $\lim_{x\to a}[x-a]=0$. Thus by
L'H\^opital's rule:
\[f'(a)=\lim_{x\to a}\frac{f'(x)}{1}=\lim_{x\to a}f'(x),\]

as desired.

%%% Local Variables:
%%% TeX-master: "notes"
%%% End:
